{"mode":"editor","version":1,"windowDimensions":{"x":45,"y":23,"width":1235,"height":777,"maximized":true},"syntax":{"deserializer":"Syntax","grammarOverridesByPath":{}},"project":{"path":"/Users/ssun/Projects/tumblr/puppet","buffers":[{"text":"# this is a separate solr infrastructure than what Search team maintains\nclass solr(instance_name, root = undef) {\n  require jdk\n  $workdir = $root ? {\n    undef   => '/var/lib/solr/example/solr/collection1',\n    default => $root,\n  }\n  $confdir = \"${workdir}/conf\"\n\n  yum::pinnedpkg { 'solr':\n    version          => \"4.8.1-2.el${::lsbmajdistrelease}.tumblr\",\n    devel_has_latest => false;\n  }\n\n  logrotate::genconf { 'solr':\n    owner => 'solr',\n    group => 'solr';\n  }\n\n  monit::genconf { 'solr':\n    require    => Service['solr'];\n  }\n\n  File {\n    owner => 'solr',\n  }\n\n  file {\n    $workdir:\n      ensure => directory,\n      mode => 0755,\n      require   => Yum::Pinnedpkg['solr'];\n    $confdir:\n      ensure => directory,\n      mode => 0755,\n      require => File[$workdir];\n    \"$confdir/lang\":\n      ensure => directory,\n      mode   => 0755,\n      require => File[$confdir];\n    \"/etc/sysconfig/solr\":\n      source  => \"puppet:///modules/solr/$instance_name/solr.sysconfig\",\n      mode    => 0644,\n      require => Yum::Pinnedpkg['solr'];\n  }\n\n  define solr::conf {\n    file { $name:\n      path    => \"$confdir/$name\",\n      mode    => 0644,\n      source  => \"puppet:///modules/solr/$instance_name/$name\",\n      owner   => 'solr',\n      before  => Service['solr'],\n      require => File[[$confdir,\"$confdir/lang\"]];\n    }\n  }\n\n  solr::conf {\n    ['elevate.xml','protwords.txt','schema.xml','solrconfig.xml','stopwords.txt','synonyms.txt','lang/stopwords_en.txt']:\n  }\n\n  service { 'solr':\n    require   => [Solr::Conf['elevate.xml','protwords.txt','schema.xml','solrconfig.xml','stopwords.txt','synonyms.txt','lang/stopwords_en.txt'],File['/etc/sysconfig/solr'],Yum::Pinnedpkg['solr']],\n    provider  => 'redhat',\n    ensure    => 'running',\n    hasstatus => true;\n  }\n\n}\n","markers":{"markers":{"1":{"id":1,"range":[[10,20],[10,20]],"tailed":false,"reversed":false,"valid":true,"invalidate":"never","persistent":true,"properties":{"type":"selection","editorId":1187,"goalBufferRange":null,"preserveFolds":true},"deserializer":"Marker"},"500":{"id":500,"range":[[40,0],[40,0]],"tailed":true,"reversed":false,"valid":true,"invalidate":"overlap","persistent":true,"properties":{},"deserializer":"Marker"}},"deserializer":"MarkerManager"},"history":{"undoStack":[],"redoStack":[],"deserializer":"History"},"encoding":"utf8","filePath":"/Users/ssun/Projects/tumblr/puppet/modules/solr/manifests/init.pp","modifiedWhenLastPersisted":false,"digestWhenLastPersisted":"606cb11fc7c16ff9fe3bf79b765c33fce50a2d31","deserializer":"TextBuffer"},{"text":"#! /opt/ruby-1.9.2/bin/ruby\n# encoding: utf-8\n\nrequire 'benchmark'\nrequire 'getoptlong'\nrequire 'tempfile'\nrequire 'net/smtp'\nrequire 'socket'\nrequire 'tsd_client'\nrequire 'logger'\nrequire 'nokogiri'\nrequire 'hdfs/classpath'\nrequire 'uri'\n\n$conf = {\n  :parallel_copies => 10,\n  :ygrid_root      => '/projects',\n  :db_root         => '/projects/tumblr/hive/table_data',\n  :partition_root  => '/projects/tumblr/hive/partitions',\n  :nn              => 'nn-pb-3192ac66.ewr01.tumblr.net',\n  :proxy           => 'https://tiberiumtan-nn1-pxy.tan.ygrid.yahoo.com:4443',\n  :cookie          => '/home/ygrid/proxycookie.txt',\n  :keytab          => '/home/ygrid/tumblr.prod.headless.keytab',\n  :tmp_location    => '/home/ygrid/',\n  :notify_list     => \"sre-data@tumblr.com\",\n  :curl            => 'curl -s --cacert /home/ygrid/ygrid_cacert.pem -b /home/ygrid/proxycookie.txt',\n\n  :log_file        => '/home/ygrid/ygrid_sync.log', # can't put this in /var/log because of permission\n  :log_retention   => 3,       # files\n  :log_rotate_size => 2 ** 28, # 256MB\n}\n$log = Logger.new $conf[:log_file], $conf[:log_retention], $conf[:log_rotate_size]\n$hdfs = Hadoop::DFS::FileSystem.new host: $conf[:nn], port: 8020\n\nopts = GetoptLong.new(\n  ['--help', '-h', GetoptLong::NO_ARGUMENT],\n  ['--name', '-n', GetoptLong::REQUIRED_ARGUMENT],\n  ['--minions', '-m', GetoptLong::REQUIRED_ARGUMENT],\n  ['--local_file', '-l', GetoptLong::REQUIRED_ARGUMENT],\n  ['--remote_file', '-r', GetoptLong::REQUIRED_ARGUMENT],\n  ['--mode', '-o', GetoptLong::REQUIRED_ARGUMENT],\n  ['--strip_length', '-s', GetoptLong::OPTIONAL_ARGUMENT],\n  ['--delta', '-d', GetoptLong::OPTIONAL_ARGUMENT],\n  ['--fancy', '-f', GetoptLong::OPTIONAL_ARGUMENT],\n  ['--target', '-t', GetoptLong::OPTIONAL_ARGUMENT],\n  ['--prep', '-p', GetoptLong::OPTIONAL_ARGUMENT],\n  ['--copy', '-c', GetoptLong::OPTIONAL_ARGUMENT],\n)\n\n$name = nil\n$local_file_name = nil\n$remote_file_name = nil\n$minions = nil\n$target = nil\n$strip_length = 1\n$fancy = false\n$copy = false\n$delta = false\n$prep = false\n$full_sync = 0\n$prog_name = File.absolute_path($0)\n$cmdline = $0 + \" \" + ARGV.join(\" \")\n\ndef help()\n  puts <<-HELP_STRING\nUsage:\n  sync_table [-h|--help] [-f|--full] -t table_name\n    -h, --help                  : print this message\n    -n, --name table_name       : the table to sync\n    -m, --minions node_list     : list of datanodes\n    -p, --prep                  : prep the copy\n    -c, --copy                  : copy the file\n    -t, --target target         : mirror the tree to target\n    -l, --local_file file_name  : the file to copy over\n    -r, --remote_file file_name : the file to copy over\n    -o, --mode                  : run mode (push/pull)\n    -s, --strip_length length   : path lengths to strip\n    -f, --fancy                 : fancy (partitioned) table\n    -d, --delta                 : do a delta sync (useful\n                                  for partitioned tables),\n                                  the default is a full sync.\nHELP_STRING\nend\n\nopts.each do |option, argument|\n  case option\n    when '--fancy'\n      $fancy = true\n    when '--delta'\n      $delta = true\n    when '--prep'\n      $prep = true\n    when '--copy'\n      $copy = true\n    when '--name'\n      $name = argument\n    when '--target'\n      $target = argument\n    when '--simple'\n      $simple = argument\n    when '--minions'\n      $minions = argument\n    when '--mode'\n        $mode = argument\n    when '--local_file'\n      $local_file_path = argument\n    when '--remote_file'\n      $remote_file_path = argument\n    when '--strip_length'\n      $strip_length = argument.to_i\n    when '--help'\n      help\n      exit\n  end\nend\n\ndef auth\n  %x(klist -s)\n  if $?.exitstatus != 0\n    $log.warn(\"Tokens expired, initializing Kerberos.\")\n    %x(kinit -k -t #{$conf[:keytab]} tumblr@YGRID.YAHOO.COM)\n  end\n  %x(#{$conf[:curl]} -c #{$conf[:cookie]} -u : --negotiate -k #{$conf[:proxy]}/fs/user/tumblr)\nend\n\n# curl call - parse output and log on exceptions.\ndef cc cargs, log_error = true\n  auth\n  output = %x(#{$conf[:curl]} #{cargs})\n  if output.downcase.include? 'exception' and log_error\n    $log.error(\"Curl call #{$conf[:curl]} #{cargs} failed with output: #{output}\")\n  end\n  output\nend\n\ndef get_datanodes nn\n  nodes = %x[curl -s http://#{nn}:50070/dfsnodelist.jsp?whatNodes=LIVE]\n  if $?.exitstatus != 0\n    notify_and_abort \"unable to obtain data node list from http://#{nn}:50070/dfsnodelist.jsp?whatNodes=LIVE\"\n  end\n  nodes.split(/\\n/).map do |line|\n    mo = line.match(/.*href=\"http:\\/\\/(dn-[^:]+):.+/)\n    if mo\n      mo[1]\n    end\n  end.compact\nend\n\ndef notify_and_abort msg = \"\"\n  from = \"ygrid@\" + Socket.gethostname;\n  smtp_message = <<MESSAGE_END\nFrom: #{from}\nTo: #{$conf[:notify_list]}\nSubject: Table/Tree copy to ygrid failed.\n\n#{msg}\nScript was invoked as: #{$cmdline}\nMESSAGE_END\n  Net::SMTP.start('localhost') do |smtp|\n    smtp.send_message smtp_message, from, $conf[:notify_list]\n  end\n  $stderr.puts msg\n  $log.fatal msg\n  exit 1\nend\n\ndef move_tree_into_place(source, destination, permission = \"770\", group = \"tumblr\")\n  $log.info 'moving tree into place for ' + $name\n  cc \"-X PUT '#{$conf[:proxy]}/fs#{destination}?op=delete&recursive=true&skipTrash=true'\", false\n  move_commands = [\"-X PUT '#{$conf[:proxy]}/fs#{source}?op=move&dest=#{destination}'\",\n                   \"-X PUT '#{$conf[:proxy]}/fs#{destination}?op=chown&recursive=true&owner=tumblr&group=#{group}'\",\n                   \"-X PUT '#{$conf[:proxy]}/fs#{destination}?op=chmod&recursive=true&permission=#{permission}'\"]\n  move_commands.all? do |ccmd|\n    cc ccmd\n  end\nend\n\ndef full_push(table_path, destination, permission = \"770\", group = \"tumblr\")\n  if table_path == \"/\"\n    notify_and_abort 'Human, I am NOT going to sync / for you.  Fix your input!'\n  end\n\n  $log.info 'retrieving table files from HDFS: ' + table_path\n\n  table_files = get_hdfs_files(table_path).select(&:is_file?).map do |entry|\n    URI::parse(entry.name).path.sub(table_path, '')\n  end\n\n  temp_file = Tempfile.new(\"/tmp/\")\n  get_datanodes($conf[:nn]).each {|node|\n    temp_file.puts node\n  }\n  temp_file.flush\n  remote_root = destination + \"_tmp\"\n  cc \"-X PUT '#{$conf[:proxy]}/fs#{remote_root}?op=delete&recursive=true&skipTrash=true'\", false\n  IO.popen('-', 'w+') {|cpipe|\n    if cpipe\n      # In the parent, pipe the file list to be copied into the child\n      # xargs\n      table_files.each { |file_name|\n        cpipe.puts file_name\n      }\n      cpipe.close_write\n    else\n      # In the child, exec an xargs and kick off copies.\n      $log.info 'kicking off remote jobs on datanodes to transfer data'\n      exec(\"xargs -n 1 -I{} -P #{$conf[:parallel_copies]} -- #{$prog_name} -o push -p -n #{$name} -m #{temp_file.path} -l #{table_path}{} -r /fs#{remote_root}{}\")\n    end\n  }\n  exit_status = $?.exitstatus\n  temp_file.close\n  if exit_status != 0\n    notify_and_abort \"Non zero exit from one of the child processes while copying #{table_path}.\"\n  else\n    unless move_tree_into_place(remote_root, destination, permission, group)\n      notify_and_abort \"Unable to move tree into place at #{remote_root}\"\n    end\n  end\nend\n\ndef copy_hdfs_file(local, remote, mode)\n  exit_status = 0\n  if mode == 'push'\n    auth\n    output = %x[hadoop fs -cat #{local} | #{$conf[:curl]} -T - #{$conf[:proxy]}#{remote}?op=create&overwrite=true]\n    if output.downcase.include? 'exception'\n      $log.info \"Curl call failed for pushing file #{local}, output: #{output}\"\n      exit_status += 1\n    end\n  elsif mode == 'pull'\n    begin\n      target_file = $hdfs.open(local, 'w')\n      auth\n      bytes_copied = IO.popen(\"#{$conf[:curl]} #{$conf[:proxy]}#{remote}\") do |source|\n        IO.copy_stream source, target_file\n      end\n      target_file.close if target_file.write_open? rescue nil\n      if bytes_copied < 500\n        f = $hdfs.open(local)\n        if f.read.include? 'AccessDeniedException'\n          $log.error \"accessed denied pulling file #{remote} from ygrid\"\n          exit_status += 1\n        end\n        f.close if f.read_open? rescue nil\n      end\n    rescue\n      $log.error \"error pulling file #{remote} from ygrid\"\n      exit_status += 1\n    end\n  end\n  exit exit_status\nend\n\ndef prep_copy_file(node_list_file, local_file_path, remote_file_path, mode)\n  node = IO.readlines(node_list_file).sample.strip\n  exit_status = 0\n  time_taken = Benchmark.realtime do\n    %x(ssh -i /home/ygrid/id_rsa_ygrid -o StrictHostKeyChecking=no #{node} '#{$prog_name} -o #{mode} -c -n #{$name} -l #{local_file_path} -r #{remote_file_path}')\n    exit_status = $?.exitstatus\n  end\n  if exit_status == 0\n    file_size = $hdfs.stat(local_file_path).size\n    tsd = TSD::Client.new host: 'opentsdb.ewr01.tumblr.net', port: 4243\n    tsd.put metric: 'ygrid_transfer.rate',\n            value: (file_size.to_i / time_taken),\n            tags: {host: node, table_id: File.basename($name), mode: mode}\n  else\n    $log.error \"failed to #{mode} #{$name} on #{node}\"\n  end\n  exit exit_status\nend\n\n# this function queries ygrid recursively for all files in a path\n# return a hash with directories as keys and files in the directory as values\ndef get_ygrid_files path\n  out = cc \"#{$conf[:proxy]}/fs#{path}?op=status\"\n  files = Nokogiri::XML(out).xpath('//file').map {|entry| entry['path']}\n  res = {path => files}\n  Nokogiri::XML(out).xpath('//directory').reject do |entry|\n    entry['path'] == path\n  end.each do |entry|\n    res = res.merge(get_ygrid_files(entry['path']))\n  end\n  Hash[res.sort_by{|k,v| k.length}]\nend\n\ndef get_hdfs_files path\n  $hdfs.list_directory(path).map do |entry|\n    if entry.is_directory?\n      [entry, get_hdfs_files(entry.name)]\n    else\n      entry\n    end\n  end.flatten\nend\n\ndef full_pull(ygrid_path, tumblr_path)\n  unless ygrid_path.start_with?  $conf[:ygrid_root]\n    notify_and_abort \"Only data located in #{conf[:ygrid_root]} can be pulled!\"\n  end\n\n  unless tumblr_path.start_with? '/ygrid'\n    notify_and_abort \"Downloaded data can only be placed under /ygrid!\"\n  end\n\n  # check if the destination dir already exists\n  notify_and_abort \"HDFS dir #{tumblr_path} already exists!\" if $hdfs.exist? tumblr_path\n\n  $log.info 'retrieving files list from ygrid: ' + ygrid_path\n  remote_files = get_ygrid_files ygrid_path\n\n  tumblr_tmp_path = tumblr_path + \"_tmp\"\n  $hdfs.delete(tumblr_tmp_path, true) if $hdfs.exist? tumblr_tmp_path\n  remote_files.keys.each do |dir|\n     local_dir = dir.sub(ygrid_path, tumblr_tmp_path)\n     $log.info 'creating hdfs directory: ' + local_dir\n     $hdfs.create_directory local_dir\n  end\n\n  temp_file = Tempfile.new(\"/tmp/\")\n  get_datanodes($conf[:nn]).each {|node|\n    temp_file.puts node\n  }\n  temp_file.flush\n\n  IO.popen('-', 'w+') {|cpipe|\n    if cpipe\n      # In the parent, pipe the file list to be pulled into the child\n      # xargs\n      remote_files.values.flatten.each { |file_name|\n        if file_name == ygrid_path # this means we are downloading a single file\n          cpipe.puts File.basename('/'+file_name)\n        else\n          cpipe.puts file_name.sub(ygrid_path, '')\n        end\n      }\n      cpipe.close_write\n    else\n      # In the child, exec an xargs and kick off copies.\n      $log.info 'kicking off remote jobs on datanodes to pull data'\n      exec(\"xargs -n 1 -I{} -P #{$conf[:parallel_copies]} -- #{$prog_name} -o pull -p -n #{$name} -m #{temp_file.path} -l #{tumblr_tmp_path}{} -r /fs#{ygrid_path}{}\")\n    end\n  }\n  exit_status = $?.exitstatus\n  temp_file.close\n  if exit_status != 0\n    notify_and_abort \"Non zero exit from one of the child processes while pulling #{ygrid_path}.\"\n  else\n    $log.info \"moving #{tumblr_tmp_path} to #{tumblr_path}\"\n    $hdfs.rename tumblr_tmp_path, tumblr_path\n  end\nend\n\ndef main()\n  unless ($copy or $prep)\n    if $name == nil\n      help\n      exit 1\n    else\n      case $mode\n      when 'push'\n        $log.info 'start pushing to ygrid: ' + $name\n        table_path = \"/\"\n        remote_path = \"\"\n        remote_root = \"\"\n        if $name =~ /^\\//\n          table_path = $name\n          remote_path = $target\n        else\n          $log.info 'fetching hive table info for ' + $name\n          if $fancy\n            a_partition = %x[hive cmdline -e 'show partitions #{$name}' | head -1 | sed -e 's/=/=\"/' -e 's/$/\"/']\n            partition_info = %x[hive cmdline -e 'show table extended like #{$name} partition(#{a_partition.strip});']\n            mo = partition_info.match(/location.hdfs:.+:\\d+(\\/.+)\\n/)\n            table_path = mo[1].strip.split(\"/\")[0..-$strip_length].join(\"/\")\n            remote_root = $conf[:partition_root]\n          else\n            table_description = %x[hive cmdline -e 'describe formatted #{$name}']\n            mo = table_description.match(/Location:.*hdfs...(.*):\\d+(\\/.+)\\n/)\n            if mo\n              table_path = mo[2].strip\n              remote_root = $conf[:db_root]\n            end\n          end\n          remote_path = remote_root + \"/\" + File.basename(table_path)\n          if $target\n            remote_path = $target\n          end\n        end\n        permission = \"770\"\n        group = \"tumblr\"\n        if remote_path =~ /exports/\n          permission = \"750\"\n          group = \"tumblr_shared\"\n        end\n        if File.basename(table_path) != \"\"\n          full_push(table_path, remote_path, permission, group)\n          $log.info \"successfully transfered #{$name} to ygrid\"\n        else\n          notify_and_abort \"Unable to figure out source table path - #{$name}.\"\n        end\n      when 'pull'\n        $log.info 'start pulling from ygrid: ' + $name\n        full_pull($name, $target)\n        $log.info \"successfully pulled #{$name} from ygrid\"\n      else\n        abort 'wrong mode given, only push/pull supported!'\n      end\n    end\n  else\n    if $prep\n      prep_copy_file($minions, $local_file_path, $remote_file_path, $mode)\n    elsif $copy\n      copy_hdfs_file($local_file_path, $remote_file_path, $mode)\n    end\n  end\nend\n\nmain\n","markers":{"markers":{"1":{"id":1,"range":[[239,11],[239,11]],"tailed":false,"reversed":false,"valid":true,"invalidate":"never","persistent":true,"properties":{"type":"selection","editorId":1239,"goalBufferRange":null,"preserveFolds":true},"deserializer":"Marker"},"16":{"id":16,"range":[[239,11],[239,11]],"tailed":true,"reversed":false,"valid":true,"invalidate":"overlap","persistent":true,"properties":{},"deserializer":"Marker"},"17":{"id":17,"range":[[228,0],[228,7]],"tailed":true,"reversed":false,"valid":true,"invalidate":"never","persistent":true,"properties":{},"deserializer":"Marker"}},"deserializer":"MarkerManager"},"history":{"undoStack":[{"patches":[{"oldRange":[[223,0],[223,0]],"newRange":[[223,0],[225,0]],"oldText":"","newText":"    auth\n    output = %x[hadoop fs -cat #{local} | #{$conf[:curl]} -T - #{$conf[:proxy]}#{remote}?op=create&overwrite=true]\n","normalizeLineEndings":false,"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[225,0],[229,0]],"newRange":[[225,0],[225,0]],"oldText":"    temp_file = Dir::Tmpname.create($conf[:tmp_location]) {|path| path}\n    %x(hdfs dfs -get #{local} #{temp_file})\n    exit_status += $?.exitstatus\n    output = cc \"-T #{temp_file} '#{$conf[:proxy]}#{remote}?op=create&overwrite=true'\"\n","newText":"","normalizeLineEndings":false,"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[226,0],[227,0]],"newRange":[[226,0],[226,0]],"oldText":"    File.unlink temp_file\n","newText":"","normalizeLineEndings":false,"markerPatches":{},"deserializer":"BufferPatch"}],"deserializer":"Transaction"},{"patches":[{"oldRange":[[225,0],[225,0]],"newRange":[[225,0],[228,0]],"oldText":"","newText":"    if output.downcase.include? 'exception'\n      $log.info \"Curl call failed for pushing file #{local}, output: #{output}\"\n      exit_status += 1\n","normalizeLineEndings":false,"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[228,0],[229,0]],"newRange":[[228,0],[228,0]],"oldText":"    exit_status += 1 if output.downcase.include? 'exception'\n","newText":"","normalizeLineEndings":false,"markerPatches":{},"deserializer":"BufferPatch"}],"deserializer":"Transaction"},{"patches":[{"oldRange":[[228,0],[228,0]],"newRange":[[228,0],[229,0]],"oldText":"","newText":"    end\n","normalizeLineEndings":false,"markerPatches":{},"deserializer":"BufferPatch"}],"deserializer":"Transaction"}],"redoStack":[],"deserializer":"History"},"encoding":"utf8","filePath":"/Users/ssun/Projects/tumblr/puppet/modules/hadoop/files/sync_table.rb","modifiedWhenLastPersisted":false,"digestWhenLastPersisted":"bb79836dcc386467126d2f60e939131420dbbb61","deserializer":"TextBuffer"},{"text":"#!/usr/bin/env tumblr_ruby\nrequire 'zk'\nrequire 'json'\nrequire 'logger'\nrequire 'socket'\nrequire 'timeout'\nrequire 'nrpe_check'\n\nclass CheckHBase\n  include NRPE::Check\n  attr_reader :options, :log\n\n  def initialize options = {}\n    @options = {\n      timeout:            120,  # seconds\n      critical_threshold: 1800, # amount of time (seconds) check should be in warning status\n                                # before escalating to critical status\n\n      log: {\n        location:  '/var/log/hbase/check_hbase.log',\n        read_size: 2 ** 20, # 1mb\n        max_size:  2 ** 30, # 1gb\n        retention: 3, # files\n        format:    lambda {|severity, time, program_name, message| message.merge(time: time.to_i).to_json + \"\\n\"},\n      },\n\n      zookeeper: {\n        znode: '/hbase/master',\n        host:  '0.0.0.0',\n        port:  2181,\n      },\n\n      hbck: {\n        command: 'hbase hbck 2>/dev/null',\n        record_path: File.join('/tmp', [:hbck, Time.now.to_i].join('_')),\n        keys: [\n          'Version',\n          'Master',\n          'Status',\n          'Number of Tables',\n          'Number of live region servers',\n          'Number of dead region servers',\n          'Number of backup masters',\n        ],\n      },      \n    }.merge options\n\n    @log = Logger.new @options[:log][:location], \n                      @options[:log][:retention], \n                      @options[:log][:max_size]\n    @log.formatter =  @options[:log][:format]\n  end\n\n  def active_master?\n    ZK.open [options[:zookeeper][:host], options[:zookeeper][:port]].join(':') do |zk|\n      if zk.exists? options[:zookeeper][:znode]\n        value, status = zk.get options[:zookeeper][:znode]\n\n        value.include? Socket.gethostname\n      end\n    end\n  end\n\n  def status_log_entries(offset = File.size(options[:log][:location]) - options[:log][:read_size])\n    # ensure valid offset\n    offset = 0 if offset < 0\n\n    # read the last n bytes of status log\n    File.read(options[:log][:location],\n              options[:log][:read_size],\n              offset).lines.reduce([]) do |entries, line|\n\n      # parse entry if valid json\n      if line =~ /^{.+}$/\n        entry = JSON.parse line, symbolize_names: true\n\n        # filter entries from within our time threshold\n        if Time.now - Time.at(entry[:time]) < options[:critical_threshold]\n          entries << entry\n        end\n      end\n\n      entries\n    end\n  end\n\n  def escalate_status?\n    entries = status_log_entries\n    not entries.empty? and entries.all? {|entry| entry[:status] == :warning}\n  end\n\n  def hbck\n    @hbck ||= Timeout.timeout options[:timeout] do\n      [%x[#{options[:hbck][:command]}], $?]\n    end\n\n    Hash[[:output, :status].zip(@hbck)]\n  rescue Timeout::Error\n    exit_with_status :critical, 'hbck timeout'\n  end\n\n  def hbase_status\n    @hbase_status ||= hbck[:output].lines.reduce({}) do |status, line|\n      if options[:hbck][:keys].any? {|key| line.include? key}\n        key, value  = line.chomp.split ':'\n        status[key] = value.strip\n      end\n\n      status\n    end\n  end\n\n  def run\n    check do\n      status :unknown, 'unable to determine hbck status' # register an initial status\n\n      # exit if in standby state\n      exit_with_status :ok, 'not active master, skipping check' unless active_master?\n\n      # check hbase\n      check_status, check_message = case\n      when hbase_status['Number of dead region servers'].to_i != 0\n        [:critical, \"dead region servers: #{hbase_status['Number of dead region servers']}\"]\n      when hbase_status['Status'] != 'OK'\n        [:warning, \"hbck status: #{hbase_status['Status']}\"]\n      when hbck[:status].exitstatus != 0\n        [:warning, \"hbck status: unhealthy\"]\n      else\n        [:ok, hbase_status.to_json]\n      end\n\n      unless check_status == :ok\n        # record raw hbck output\n        unless hbck[:output].empty?\n          File.open options[:hbck][:record_path], 'w' do |file|\n            file.write hbck[:output]\n          end\n        end\n\n        # escalate status if in prolonged warning state\n        if check_status == :warning and escalate_status?\n          check_status = :critical\n        end\n      end\n\n      log.info status: check_status\n      status check_status, check_message\n    end\n  end\nend\n\nCheckHBase.new.run\n\n\n","markers":{"markers":{"1":{"id":1,"range":[[96,26],[96,26]],"tailed":false,"reversed":false,"valid":true,"invalidate":"never","persistent":true,"properties":{"type":"selection","editorId":1275,"goalBufferRange":null,"preserveFolds":true},"deserializer":"Marker"},"599":{"id":599,"range":[[56,0],[56,58]],"tailed":true,"reversed":false,"valid":true,"invalidate":"never","persistent":true,"properties":{},"deserializer":"Marker"}},"deserializer":"MarkerManager"},"history":{"undoStack":[],"redoStack":[{"patches":[{"oldRange":[[15,38],[15,39]],"newRange":[[15,38],[15,38]],"oldText":"a","newText":"","normalizeLineEndings":{},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[15,37],[15,38]],"newRange":[[15,37],[15,37]],"oldText":"a","newText":"","normalizeLineEndings":{},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[15,36],[15,37]],"newRange":[[15,36],[15,36]],"oldText":"a","newText":"","normalizeLineEndings":{},"markerPatches":{},"deserializer":"BufferPatch"}],"deserializer":"Transaction"}],"deserializer":"History"},"encoding":"utf8","filePath":"/Users/ssun/Projects/tumblr/puppet/modules/nrpe/files/contrib-checks/check_hbase.rb","modifiedWhenLastPersisted":false,"digestWhenLastPersisted":"550c58130586d085db61381954b72b1c3122bb65","deserializer":"TextBuffer"},{"text":"","markers":{"markers":{"1":{"id":1,"range":[[0,0],[0,0]],"tailed":false,"reversed":false,"valid":true,"invalidate":"never","persistent":true,"properties":{"type":"selection","editorId":1267,"goalBufferRange":null},"deserializer":"Marker"}},"deserializer":"MarkerManager"},"history":{"undoStack":[],"redoStack":[],"deserializer":"History"},"encoding":"utf8","modifiedWhenLastPersisted":false,"deserializer":"TextBuffer"}],"deserializer":"Project"},"workspace":{"paneContainer":{"root":{"id":45,"items":[{"id":1239,"softTabs":true,"displayBuffer":{"id":1240,"softWrapped":false,"editorWidthInChars":null,"scrollTop":4372,"scrollLeft":0,"tokenizedBuffer":{"bufferPath":"/Users/ssun/Projects/tumblr/puppet/modules/hadoop/files/sync_table.rb","invisibles":null,"deserializer":"TokenizedBuffer"},"invisibles":null,"deserializer":"DisplayBuffer"},"deserializer":"TextEditor"},{"id":1275,"softTabs":true,"displayBuffer":{"id":1276,"softWrapped":false,"editorWidthInChars":null,"scrollTop":1182,"scrollLeft":0,"tokenizedBuffer":{"bufferPath":"/Users/ssun/Projects/tumblr/puppet/modules/nrpe/files/contrib-checks/check_hbase.rb","invisibles":null,"deserializer":"TokenizedBuffer"},"invisibles":null,"deserializer":"DisplayBuffer"},"deserializer":"TextEditor"},{"id":1267,"softTabs":true,"displayBuffer":{"id":1268,"softWrapped":false,"editorWidthInChars":null,"scrollTop":0,"scrollLeft":0,"tokenizedBuffer":{"invisibles":null,"deserializer":"TokenizedBuffer"},"invisibles":null,"deserializer":"DisplayBuffer"},"deserializer":"TextEditor"},{"id":1187,"softTabs":true,"displayBuffer":{"id":1188,"softWrapped":false,"editorWidthInChars":null,"scrollTop":0,"scrollLeft":0,"tokenizedBuffer":{"bufferPath":"/Users/ssun/Projects/tumblr/puppet/modules/solr/manifests/init.pp","invisibles":null,"deserializer":"TokenizedBuffer"},"invisibles":null,"deserializer":"DisplayBuffer"},"deserializer":"TextEditor"}],"activeItemUri":"/Users/ssun/Projects/tumblr/puppet/modules/hadoop/files/sync_table.rb","focused":false,"deserializer":"Pane"},"activePaneId":45,"deserializer":"PaneContainer","version":1},"fullScreen":false,"packagesWithActiveGrammars":["language-ruby","language-puppet","language-hyperlink","language-todo"],"deserializer":"Workspace"},"packageStates":{"fuzzy-finder":{"/Users/ssun/Projects/tumblr/puppet/modules/hadoop/files/sync_table.rb":1416348779049,"/Users/ssun/Projects/tumblr/puppet/modules/nrpe/files/contrib-checks/check_hbase.rb":1415645977509,"/Users/ssun/Projects/tumblr/puppet/modules/solr/manifests/init.pp":1415309348529},"keybinding-resolver":{"attached":false},"metrics":{"sessionLength":71888140},"tree-view":{"directoryExpansionStates":{"modules":{"configrr":{"manifests":{},"templates":{}},"hadoop":{"manifests":{}},"solr":{"files":{"collins":{}},"manifests":{}}}},"hasFocus":false,"attached":true,"scrollLeft":9,"scrollTop":5583,"width":199},"term2":{"termViews":["","",""]},"find-and-replace":{"viewState":"","modelState":{"useRegex":false,"inCurrentSelection":false,"caseSensitive":false},"projectViewState":"","resultsModelState":{"useRegex":false,"caseSensitive":false},"findHistory":["size(","::collins","define yum::pinnedpkg","logrotate::genconf","define logrotate::genconf","define monit::genconf","hadoop::ygrid_crons","decomm","refrog","decomm","logserver","hostname","splunk","let(:params)","snmpclients","oncall_notes_creds","api_key","$?","$? ","pull","mode","export","record_path","readline","read(30"],"replaceHistory":[],"pathsHistory":[]},"sublime-tabs":{"directoryExpansionStates":{},"selectedPath":"/Users/ssun/Projects/tumblr/puppet/modules","hasFocus":false,"attached":true,"scrollLeft":0,"scrollTop":0,"width":196}}}